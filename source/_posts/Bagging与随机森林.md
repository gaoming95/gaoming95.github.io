---
title: Bagging与随机森林
subtitle: Bagging与随机森林
date: 2019-08-03
author: 高明
tags:
	- 机器学习
---



# Bagging与随机森林

在[集成学习原理小结](http://www.cnblogs.com/pinard/p/6131423.html)中，我们讲到了集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。本文就对集成学习中Bagging与随机森林算法做一个总结。

随机森林是集成学习中可以和梯度提升树GBDT分庭抗礼的算法，尤其是它可以很方便的并行训练，在如今大数据大样本的的时代很有诱惑力

## bagging的原理

在[集成学习原理小结](http://www.cnblogs.com/pinard/p/6131423.html)中，我们给Bagging画了下面一张原理图。

![m 个 样 本  弱 学 习 器 1  采 样 集 1  m 个 样 本  弱 学 习 器 2  采 样 集 2  强 学 习 器  亠 合 策 ，  机 采 羊  m 个 样 本  训 练 集  m 个 样 本  弱 学 习 器 T  刘 建 平 訕 d  采 样 集 T ](file:///C:/Users/gaoming/AppData/Local/Temp/msohtmlclip1/01/clip_image001.png)

从上图可以看出，Bagging的弱学习器之间的确没有boosting那样的联系。它的特点在“随机采样”。那么什么是随机采样？

 随机采样(bootsrap)就是从我们的训练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于我们的Bagging算法，一般会随机采集和训练集样本数m一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有m个样本训练集做T次的随机采样，则由于随机性，T个采样集各不相同。

 注意到这和GBDT的子采样是不同的。GBDT的子采样是无放回采样，而Bagging的子采样是放回采样。

 对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是。不被采集到的概率为1。如果m次采样都没有被采集中的概率是()m。当m→∞时，()m→≃0.368。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。![img](file:///C:/Users/gaoming/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png)

bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。bagging的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏倚会大一些。

## bagging算法流程

上一节我们对bagging算法的原理做了总结，这里就对bagging算法的流程做一个总结。相对于Boosting系列的Adaboost和GBDT，bagging算法要简单的多。

输入为样本集弱学习器算法, 弱分类器迭代次数T。![img](file:///C:/Users/gaoming/AppData/Local/Temp/msohtmlclip1/01/clip_image003.png)

输出为最终的强分类器![img](file:///C:/Users/gaoming/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png)

1. 对于t=1,2...,T:
   1. 对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dt
   2. 用采样集Dt训练第t个弱学习器Gt(x)
2. 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

## 随机森林算法

理解了bagging算法，随机森林(Random Forest,以下简称RF)就好理解了。它是Bagging算法的进化版，也就是说，它的思想仍然是bagging,但是进行了独有的改进。我们现在就来看看RF算法改进了什么。

首先，RF使用了CART决策树作为弱学习器，这让我们想到了梯度提升树GBDT。第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为**n****sub** ，然后在这些随机选择的**n****sub** 个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。

如果**n****sub** =n，则此时RF的CART决策树和普通的CART决策树没有区别。**n****sub** 越小，则模型越健壮，当然此时对于训练集的拟合程度会变差。也就是说**n****sub** 越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的**n****sub** 的值。

除了上面两点，RF和普通的bagging算法没有什么不同， 下面简单总结下RF的算法。

输入为样本集，弱分类器迭代次数T。![img](file:///C:/Users/gaoming/AppData/Local/Temp/msohtmlclip1/01/clip_image005.png)

输出为最终的强分类器![img](file:///C:/Users/gaoming/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png)

1. 对于t=1,2...,T:
   1. 对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dt
   2. 用采样集Dt训练第t个决策树模型Gt(x)，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分

2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

## 随机森林的推广

由于RF在实际应用中的良好特性，基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。下面对于这些RF家族的算法中有代表性的做一个总结。

### extra trees

extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：

1.  对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。
2. 在选定了划分特征后，RF的决策树会基于基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树

从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是偏倚相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好

### Totally Random Trees Embedding

Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。

TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征xx划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0,   0,0,1,0,0,   0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。

映射到高维特征后，可以继续使用监督学习的各种分类回归算法了

## 随机森林小结

RF的算法原理也终于讲完了，作为一个可以高度并行化的算法，RF在大数据时候大有可为。 这里也对常规的随机森林算法的优缺点做一个总结。

RF的主要优点有

1. 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是最主要优点
2. 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型
3.  在训练后，可以给出各个特征对于输出的重要性
4. 由于采用了随机采样，训练出的模型的方差小，泛化能力强
5. 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单
6. 对部分特征缺失不敏感

### RF的主要缺点

1. 在某些噪音比较大的样本集上，RF模型容易陷入过拟合
2. 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果

###  随机森林和GBDT区别

1. 随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。
2. 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
3. 组成随机森林的树可以并行生成；而GBDT只能是串行生成。
4. 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
5. 随机森林对异常值不敏感；GBDT对异常值非常敏感。
6. 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。
7. 随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。

## 参考文献

[Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)